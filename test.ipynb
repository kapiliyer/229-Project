{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "para_train = pd.read_csv('./data/quora-train.csv', sep=\"\\t\")\n",
    "sts_train = pd.read_csv('./data/sts-train.csv', sep=\"\\t\")\n",
    "para_train = para_train.dropna()\n",
    "para_train = para_train.head(1000) #!!!!\n",
    "sts_train = sts_train.dropna()\n",
    "para_dev = pd.read_csv('./data/quora-dev.csv', sep=\"\\t\")\n",
    "sts_dev = pd.read_csv('./data/sts-dev.csv', sep=\"\\t\")\n",
    "para_dev = para_dev.dropna()\n",
    "para_dev = para_dev.head(10) #!!!!\n",
    "sts_dev = sts_dev.dropna() \n",
    "para_test = pd.read_csv('./data/quora-test-student.csv', sep=\"\\t\")\n",
    "sts_test = pd.read_csv('./data/sts-test-student.csv', sep=\"\\t\")\n",
    "para_test = para_test.dropna()\n",
    "sts_test = sts_test.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cs229/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)\n",
    "# print(para_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "N_PARAPHRASE_CLASSES = 1\n",
    "N_SIMILARITY_CLASSES = 5\n",
    "DROPOUT_PROB = 0.5\n",
    "INPUT_SIZE = 768\n",
    "\n",
    "class NLP_Model(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NLP_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.paraphrase_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.paraphrase_linear_interact = nn.Linear(INPUT_SIZE, N_PARAPHRASE_CLASSES)\n",
    "        self.similarity_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.similarity_linear_interact = nn.Linear(INPUT_SIZE, N_SIMILARITY_CLASSES)\n",
    "    \n",
    "    def forward(self, sentences1, sentences2, task, device):\n",
    "        '''\n",
    "        Task 0 is para. Task 1 is similarity.\n",
    "        '''\n",
    "        sentences1 = torch.as_tensor(self.model.encode(sentences1.tolist()))\n",
    "        sentences1 = sentences1.to(device)\n",
    "        sentences2 = torch.as_tensor(self.model.encode(sentences2.tolist()))\n",
    "        sentences2 = sentences2.to(device)\n",
    "        if task == 0:\n",
    "            sentences1 = self.dropout(sentences1)\n",
    "            sentences1 = F.relu(self.paraphrase_linear(sentences1))\n",
    "            sentences2 = self.dropout(sentences2)\n",
    "            sentences2 = F.relu(self.paraphrase_linear(sentences2))\n",
    "            combined = torch.concat((sentences1, sentences2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.sigmoid(self.paraphrase_linear_interact(combined))\n",
    "        if task == 1:\n",
    "            sentences1 = self.dropout(sentences1)\n",
    "            sentences1 = F.relu(self.similarity_linear(sentences1))\n",
    "            sentences2 = self.dropout(sentences2)\n",
    "            sentences2 = F.relu(self.similarity_linear(sentences2))\n",
    "            combined = torch.concat((sentences1, sentences2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.softmax(self.similarity_linear_interact(combined), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def save_model(model, optimizer, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, f'{filepath}/model')\n",
    "    model.model.save(f'{filepath}/transformer')\n",
    "    print(f\"saved the model to {filepath}\")\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    with torch.no_grad():\n",
    "        save_info = torch.load(f'{filepath}/model')\n",
    "        transformer_model = SentenceTransformer(f'{filepath}/transformer')\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        model = NLP_Model(transformer_model)\n",
    "        model.load_state_dict(save_info['model'])\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = AdamW(list(model.parameters()) + list(transformer_model.parameters()), lr=1e-4)\n",
    "        optimizer.load_state_dict(save_info['optim'])\n",
    "        \n",
    "        random.setstate(save_info['system_rng'])\n",
    "        np.random.set_state(save_info['numpy_rng'])\n",
    "        torch.random.set_rng_state(save_info['torch_rng'])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def get_batches(dataset, batch_size=256):\n",
    "    \"\"\"\n",
    "    Pass in dataset and batch size.\n",
    "    Get generator which yields batches.\n",
    "    \"\"\"\n",
    "    return enumerate(dataset[i*batch_size:(i+1)*batch_size] for i in range(ceil(dataset.shape[0] / batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "def train_singletask_para_model(para_train, para_dev, filepath):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-4) #~SGD with weight decay 0.01\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_accuracy = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_accuracy = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_accuracy}, para dev accuracy: {dev_para_accuracy}\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for step, batch in tqdm(get_batches(para_train), desc='train'):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['is_duplicate']\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(b_sentences1, b_sentences2, 0, device).flatten()\n",
    "\n",
    "            b_labels = torch.as_tensor(b_labels.values, dtype=torch.float32)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            loss = F.binary_cross_entropy(logits, b_labels, reduction='mean')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_para_accuracy = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_accuracy = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_accuracy}, para dev accuracy: {dev_para_accuracy}\")\n",
    "\n",
    "        if dev_para_accuracy > best_dev_acc:\n",
    "            best_dev_acc = dev_para_accuracy\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, optimizer, filepath)\n",
    "\n",
    "\n",
    "def train_singletask_sts_model():\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    multi-class cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    sts_train_dataloader = DataLoader(sts_train, shuffle=True, batch_size=16)\n",
    "    sts_dev_dataloader = DataLoader(sts_dev, shuffle=True, batch_size=16)\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "    #transformer.train()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training#train-in-native-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_singletask_model(model, device, dataset, task, flag):\n",
    "    '''\n",
    "    given dataloader, 2 task-specific finetuned models, and device\n",
    "    return the accuracy for para and for sts\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        truth = []\n",
    "        predictions = []\n",
    "        for step, batch in tqdm(get_batches(dataset), desc=f\"{flag} eval\"):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['is_duplicate']\n",
    "            truth.extend(b_labels)\n",
    "            logits = model.forward(b_sentences1, b_sentences2, task, device)\n",
    "            logits = logits.detach().cpu().numpy().flatten()\n",
    "            if task == 0:\n",
    "                new_predictions = np.round(logits).flatten()\n",
    "            else:\n",
    "                new_predictions = np.argmax(logits, axis=1).flatten()\n",
    "            predictions.extend(new_predictions)\n",
    "        accuracy = (np.array(truth).flatten() == np.array(predictions).flatten()).mean()\n",
    "    return accuracy\n",
    "\n",
    "def train_multitask_model():\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    pass \n",
    "\n",
    "def test_multitask_model():\n",
    "    '''\n",
    "    given dataloader, multitask finetuned model, and device\n",
    "    return the accuracy for para and for sts\n",
    "    '''\n",
    "    #shawty\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_singletask_model(filepath1, filepath2):\n",
    "    train_singletask_para_model(para_train, para_dev, filepath1)\n",
    "    # train_singletask_sts_model(sts_train, sts_dev, filepath2)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    para_model, _ = load_model(filepath1, device)\n",
    "    # sts_model, _ = load_model(filepath2, device)\n",
    "\n",
    "    para_acc = eval_singletask_model(para_model, device, para_test, 0, 'test')\n",
    "    # sts_acc = test_singletask_model(sts_model, device, sts_test, 1, 'test')\n",
    "\n",
    "    print(f'Final test accuracy. PARA: {para_acc}.')\n",
    "    # print(f'Final test accuracy. PARA: {para_acc}, STS: {sts_acc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train eval: 4it [00:02,  1.50it/s]\n",
      "dev eval: 1it [00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0, para train accuracy: 0.635, para dev accuracy: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 4it [00:02,  1.52it/s]\n",
      "train eval: 4it [00:02,  1.52it/s]\n",
      "dev eval: 1it [00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1, para train accuracy: 0.639, para dev accuracy: 0.7\n",
      "New best model. Saving.\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: './models/para_single/transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_singletask_model(\u001b[39m'\u001b[39;49m\u001b[39m./models/para_single\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./models/sts_single\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[126], line 2\u001b[0m, in \u001b[0;36mtest_singletask_model\u001b[0;34m(filepath1, filepath2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_singletask_model\u001b[39m(filepath1, filepath2):\n\u001b[0;32m----> 2\u001b[0m     train_singletask_para_model(para_train, para_dev, filepath1)\n\u001b[1;32m      3\u001b[0m     \u001b[39m# train_singletask_sts_model(sts_train, sts_dev, filepath2)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[124], line 49\u001b[0m, in \u001b[0;36mtrain_singletask_para_model\u001b[0;34m(para_train, para_dev, filepath)\u001b[0m\n\u001b[1;32m     47\u001b[0m best_dev_acc \u001b[39m=\u001b[39m dev_para_accuracy\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNew best model. Saving.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m save_model(model, optimizer, filepath)\n",
      "Cell \u001b[0;32mIn[118], line 13\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, optimizer, filepath)\u001b[0m\n\u001b[1;32m      4\u001b[0m save_info \u001b[39m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moptim\u001b[39m\u001b[39m'\u001b[39m: optimizer\u001b[39m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtorch_rng\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mget_rng_state(),\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m torch\u001b[39m.\u001b[39msave(save_info, filepath)\n\u001b[0;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilepath\u001b[39m}\u001b[39;49;00m\u001b[39m/transformer\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved the model to \u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/cs229/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:350\u001b[0m, in \u001b[0;36mSentenceTransformer.save\u001b[0;34m(self, path, model_name, create_model_card, train_datasets)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(path, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    352\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSave model to \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[1;32m    353\u001b[0m modules_config \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: './models/para_single/transformer'"
     ]
    }
   ],
   "source": [
    "test_singletask_model('./models/para_single', './models/sts_single')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
