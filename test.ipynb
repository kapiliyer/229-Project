{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "para_train = pd.read_csv('./data/quora-train.csv', sep=\"\\t\")\n",
    "sts_train = pd.read_csv('./data/sts-train.csv', sep=\"\\t\")\n",
    "para_train = para_train.dropna()\n",
    "sts_train = sts_train.dropna() \n",
    "para_dev = pd.read_csv('./data/quora-dev.csv', sep=\"\\t\")\n",
    "sts_dev = pd.read_csv('./data/sts-dev.csv', sep=\"\\t\")\n",
    "para_dev = para_dev.dropna()\n",
    "sts_dev = sts_dev.dropna() \n",
    "para_test = pd.read_csv('./data/quora-test.csv', sep=\"\\t\")\n",
    "sts_test = pd.read_csv('./data/sts-test.csv', sep=\"\\t\")\n",
    "para_test = para_test.dropna()\n",
    "sts_test = sts_test.dropna() \n",
    "# rand_data = sample(range(1, len(quora_df)-1), len(sts_df)-1)\n",
    "# rand_data.insert(0, 0)\n",
    "# quora_df_cut = pd.DataFrame([quora_df.iloc[i] for i in rand_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quora_1 = list(quora_df_cut['sentence1'])\n",
    "# quora_2 = list(quora_df_cut['sentence2'])\n",
    "# sts_1 = list(sts_df['sentence1'])\n",
    "# sts_2 = list(sts_df['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_groups = [quora_1, quora_2, sts_1, sts_2]\n",
    "# embeddings = [model.encode(group, show_progress_bar=True) for group in sentence_groups]\n",
    "# quora_embeddings_1 = pd.DataFrame(embeddings[0])\n",
    "# quora_embeddings_2 = pd.DataFrame(embeddings[1])\n",
    "# sts_embeddings_1 = pd.DataFrame(embeddings[2])\n",
    "# sts_embeddings_2 = pd.DataFrame(embeddings[3])\n",
    "# quora_embeddings_1.to_csv(\"quora_embeddings_1.csv\")\n",
    "# quora_embeddings_2.to_csv(\"quora_embeddings_2.csv\")\n",
    "# sts_embeddings_1.to_csv(\"sts_embeddings_1.csv\")\n",
    "# sts_embeddings_2.to_csv(\"sts_embeddings_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N_PARAPHRASE_CLASSES = 1\n",
    "N_SIMILARITY_CLASSES= 5\n",
    "DROPOUT_PROB = 0.5\n",
    "INPUT_SIZE = 768\n",
    "\n",
    "class NLP_Model(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NLP_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.paraphrase_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.paraphrase_linear_interact = nn.Linear(INPUT_SIZE, N_PARAPHRASE_CLASSES)\n",
    "        self.similarity_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.similarity_linear_interact = nn.Linear(INPUT_SIZE, N_SIMILARITY_CLASSES)\n",
    "    \n",
    "    def forward(self, sentence1, sentence2, task):\n",
    "        '''\n",
    "        Task 0 is para. Task 1 is similarity.\n",
    "        '''\n",
    "        sentence1 = torch.from_numpy(self.model.encode(sentence1))\n",
    "        sentence2 = torch.from_numpy(self.model.encode(sentence2))\n",
    "        if task == 0:\n",
    "            sentence1 = self.dropout(sentence1)\n",
    "            sentence1 = F.relu(self.paraphrase_linear(sentence1))\n",
    "            sentence2 = self.dropout(sentence2)\n",
    "            sentence2 = F.relu(self.paraphrase_linear(sentence2))\n",
    "            combined = torch.concat((sentence1, sentence2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.sigmoid(self.paraphrase_linear_interact(combined))\n",
    "        if task == 1:\n",
    "            sentence1 = self.dropout(sentence1)\n",
    "            sentence1 = F.relu(self.similarity_linear(sentence1))\n",
    "            sentence2 = self.dropout(sentence2)\n",
    "            sentence2 = F.relu(self.similarity_linear(sentence2))\n",
    "            combined = torch.concat((sentence1, sentence2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.softmax(self.similarity_linear_interact(combined), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training#train-in-native-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_singletask_model(para_filepath, sts_filepath, device, dataloader):\n",
    "    '''\n",
    "    given dataloader, 2 models, and device\n",
    "    return the accuracy for para and for sts\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        para_saved = torch.load(para_filepath)\n",
    "        sts_saved = torch.load(sts_filepath)\n",
    "        para_config = para_saved['model_config']\n",
    "        sts_config = sts_saved['model_config']\n",
    "        para_model = NLP_Model(para_config, 0)\n",
    "        sts_model = NLP_Model(sts_config, 1)\n",
    "        para_model.load_state_dict(para_saved['model'])\n",
    "        sts_model.load_state_dict(sts_saved['model'])\n",
    "        para_model = para_model.to(device)\n",
    "        sts_model = sts_model.to(device)\n",
    "        print(f\"Loaded models\")\n",
    "\n",
    "        para_test_dataloader = DataLoader(para_test, shuffle=True)\n",
    "    pass\n",
    "\n",
    "def train_multitask_model():\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    pass \n",
    "\n",
    "def test_multitask_model():\n",
    "    '''\n",
    "    just accuracy for para.\n",
    "    accuracy + MSE for five class.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "\n",
    "def train_singletask_para_model():\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    para_train_dataloader = Dataloader(para_train, shuffle=True, batch_size=16)\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.train()\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "    print(model.parameters())\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5) #SGD with weight decay 0.01\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(model.parameters())\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "def train_singletask_sts_model():\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    multi-class cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
