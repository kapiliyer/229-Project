{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "for folder in ['multitask', 'para_single', 'sts_single', 'multitask_pcgrad', 'multitask_gradnorm', 'multitask_cagrad']:\n",
    "    try:\n",
    "        os.mkdir('./models/' + folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "sts_train = pd.read_csv('./data/sts-train.csv', sep=\"\\t\")\n",
    "sts_train = sts_train.dropna()\n",
    "\n",
    "para_train = pd.read_csv('./data/quora-train.csv', sep=\"\\t\")\n",
    "para_train = para_train.dropna()[:len(sts_train)]\n",
    "\n",
    "sts_dev = pd.read_csv('./data/sts-dev.csv', sep=\"\\t\")\n",
    "sts_dev = sts_dev.dropna() \n",
    "\n",
    "para_dev = pd.read_csv('./data/quora-dev.csv', sep=\"\\t\")\n",
    "para_dev = para_dev.dropna()[:len(sts_dev)]\n",
    "\n",
    "sts_combined_set = pd.concat([sts_train, sts_dev], ignore_index=True, axis=0)\n",
    "para_combined_set = pd.concat([para_train, para_dev], ignore_index=True, axis=0)\n",
    "\n",
    "sts_train, sts_dev = train_test_split(sts_combined_set, test_size=0.3, train_size=0.7, shuffle=False)\n",
    "sts_dev, sts_test = train_test_split(sts_dev, test_size=0.5, train_size=0.5, shuffle=False)\n",
    "para_train, para_dev = train_test_split(para_combined_set, test_size=0.3, train_size=0.7, shuffle=False)\n",
    "para_dev, para_test = train_test_split(para_dev, test_size=0.5, train_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cs229/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "# print(embeddings)\n",
    "# print(para_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "N_PARAPHRASE_CLASSES = 1\n",
    "N_SIMILARITY_CLASSES = 1\n",
    "DROPOUT_PROB = 0.5\n",
    "INPUT_SIZE = 768\n",
    "\n",
    "class NLP_Model(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NLP_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.paraphrase_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.paraphrase_linear_interact = nn.Linear(INPUT_SIZE, N_PARAPHRASE_CLASSES)\n",
    "        self.similarity_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "    \n",
    "    def forward(self, sentences1, sentences2, task, device):\n",
    "        '''\n",
    "        Task 0 is para. Task 1 is similarity.\n",
    "        '''\n",
    "        sentences1 = self.model.encode(sentences1.tolist(), convert_to_tensor=True)\n",
    "        sentences1 = sentences1.to(device)\n",
    "        sentences2 = self.model.encode(sentences2.tolist(), convert_to_tensor=True)\n",
    "        sentences2 = sentences2.to(device)\n",
    "        sentences1 = self.dropout(sentences1)\n",
    "        sentences2 = self.dropout(sentences2)\n",
    "        if task == 0:\n",
    "            sentences1 = F.relu(self.paraphrase_linear(sentences1))\n",
    "            sentences2 = F.relu(self.paraphrase_linear(sentences2))\n",
    "            combined = torch.concat((sentences1, sentences2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.sigmoid(self.paraphrase_linear_interact(combined))\n",
    "        if task == 1:\n",
    "            sentences1 = F.relu(self.similarity_linear(sentences1))\n",
    "            sentences2 = F.relu(self.similarity_linear(sentences2))\n",
    "            return F.relu(F.cosine_similarity(sentences1, sentences2)) * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, f'{filepath}/model')\n",
    "    model.model.save(f'{filepath}/transformer')\n",
    "    print(f\"saved the model to {filepath}\")\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    with torch.no_grad():\n",
    "        save_info = torch.load(f'{filepath}/model')\n",
    "        transformer_model = SentenceTransformer(f'{filepath}/transformer')\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        model = NLP_Model(transformer_model)\n",
    "        model.load_state_dict(save_info['model'])\n",
    "        model.to(device)\n",
    "        \n",
    "        random.setstate(save_info['system_rng'])\n",
    "        np.random.set_state(save_info['numpy_rng'])\n",
    "        torch.random.set_rng_state(save_info['torch_rng'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def get_batches(dataset, batch_size=512):\n",
    "    \"\"\"\n",
    "    Pass in dataset and batch size.\n",
    "    Get generator which yields batches.\n",
    "    \"\"\"\n",
    "    return enumerate(dataset[i*batch_size:(i+1)*batch_size] for i in range(ceil(dataset.shape[0] / batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "def train_singletask_para_model(para_train, para_dev, filepath):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_accuracy = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_accuracy = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_accuracy}, para dev accuracy: {dev_para_accuracy}\")\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in tqdm(get_batches(para_train), desc='train'):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['is_duplicate']\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(b_sentences1, b_sentences2, 0, device).flatten()\n",
    "\n",
    "            b_labels = torch.as_tensor(b_labels.values, dtype=torch.float32)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            loss = F.binary_cross_entropy(logits, b_labels, reduction='mean')\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(total_loss / step)\n",
    "        \n",
    "        train_para_accuracy = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_accuracy = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_accuracy}, para dev accuracy: {dev_para_accuracy}\")\n",
    "\n",
    "        scheduler.step(dev_para_accuracy)\n",
    "\n",
    "        if dev_para_accuracy >= best_dev_acc:\n",
    "            best_dev_acc = dev_para_accuracy\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, filepath)\n",
    "    \n",
    "    print(losses)\n",
    "\n",
    "\n",
    "def train_singletask_sts_model(sts_train, sts_dev, filepath):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    multi-class cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in tqdm(get_batches(sts_train), desc='train'):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['similarity']\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(b_sentences1, b_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_labels = torch.as_tensor(b_labels.values, dtype=torch.float32)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            loss = F.mse_loss(logits, b_labels, reduction='mean')\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(total_loss / step)\n",
    "\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        scheduler.step(dev_sts_acc)\n",
    "\n",
    "        if dev_sts_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_sts_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, filepath)\n",
    "    \n",
    "    print(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training#train-in-native-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_singletask_model(model, device, dataset, task, flag):\n",
    "    '''\n",
    "    given dataloader, 2 task-specific finetuned models, and device\n",
    "    return the accuracy for para and for sts\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        truth = []\n",
    "        predictions = []\n",
    "        for step, batch in tqdm(get_batches(dataset), desc=f\"{flag} eval\"):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['is_duplicate' if task == 0 else 'similarity']\n",
    "            truth.extend(b_labels)\n",
    "            logits = model.forward(b_sentences1, b_sentences2, task, device)\n",
    "            logits = logits.detach().cpu().numpy().flatten()\n",
    "            if task == 0:\n",
    "                new_predictions = np.round(logits).flatten()\n",
    "            else:\n",
    "                new_predictions = logits.flatten()\n",
    "            predictions.extend(new_predictions)\n",
    "        if task == 0:\n",
    "            accuracy = (np.array(truth).flatten() == np.array(predictions).flatten()).mean()\n",
    "        else:\n",
    "            accuracy = (np.abs(np.array(truth).flatten() - np.array(predictions).flatten()) <= 0.5).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgrad import PCGrad\n",
    "\n",
    "def train_multitask_model(para_train, para_dev, sts_train, sts_dev, filepath, pcgrad_flag):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    if pcgrad_flag: optimizer = PCGrad(optimizer)\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "    dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "    print(f\"epoch number: 0, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for (para_step, para_batch), (sts_step, sts_batch) in zip(tqdm(get_batches(para_train), desc='train'), tqdm(get_batches(sts_train), desc='train')):\n",
    "            b_para_sentences1, b_para_sentences2, b_para_labels = para_batch['sentence1'], para_batch['sentence2'], para_batch['is_duplicate']\n",
    "            b_sts_sentences1, b_sts_sentences2, b_sts_labels = sts_batch['sentence1'], sts_batch['sentence2'], sts_batch['similarity']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            para_logits = model.forward(b_para_sentences1, b_para_sentences2, 0, device).flatten()\n",
    "            sts_logits = model.forward(b_sts_sentences1, b_sts_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_para_labels = torch.as_tensor(b_para_labels.values, dtype=torch.float32)\n",
    "            b_para_labels = b_para_labels.to(device)\n",
    "            b_sts_labels = torch.as_tensor(b_sts_labels.values, dtype=torch.float32)\n",
    "            b_sts_labels = b_sts_labels.to(device)\n",
    "\n",
    "            loss = (F.binary_cross_entropy(para_logits, b_para_labels, reduction='mean') + F.mse_loss(sts_logits, b_sts_labels, reduction='mean')) / 2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "        dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "        print(f\"epoch number: {epoch + 1}, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "        scheduler.step(dev_acc, 'max')\n",
    "\n",
    "        if dev_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cagrad import *\n",
    "\n",
    "def train_multitask_model_cagrad(para_train, para_dev, sts_train, sts_dev, filepath):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "    dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "    print(f\"epoch number: 0, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "    \n",
    "    linear_params = get_linear_params(model)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for (para_step, para_batch), (sts_step, sts_batch) in zip(tqdm(get_batches(para_train), desc='train'), tqdm(get_batches(sts_train), desc='train')):\n",
    "            b_para_sentences1, b_para_sentences2, b_para_labels = para_batch['sentence1'], para_batch['sentence2'], para_batch['is_duplicate']\n",
    "            b_sts_sentences1, b_sts_sentences2, b_sts_labels = sts_batch['sentence1'], sts_batch['sentence2'], sts_batch['similarity']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            para_logits = model.forward(b_para_sentences1, b_para_sentences2, 0, device).flatten()\n",
    "            sts_logits = model.forward(b_sts_sentences1, b_sts_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_para_labels = torch.as_tensor(b_para_labels.values, dtype=torch.float32)\n",
    "            b_para_labels = b_para_labels.to(device)\n",
    "            b_sts_labels = torch.as_tensor(b_sts_labels.values, dtype=torch.float32)\n",
    "            b_sts_labels = b_sts_labels.to(device)\n",
    "            \n",
    "            loss1 = F.binary_cross_entropy(para_logits, b_para_labels, reduction='mean')\n",
    "            loss1.backward()\n",
    "            para_grad = get_1d_grads(linear_params)\n",
    "            \n",
    "            loss2 = F.mse_loss(sts_logits, b_sts_labels, reduction='mean')\n",
    "            loss2.backward()\n",
    "            sts_grad = get_1d_grads(linear_params)\n",
    "            \n",
    "            new_grad = cagrad(para_grad, sts_grad)\n",
    "            apply_1d_grad(new_grad, linear_params)\n",
    "            optimizer.step()\n",
    "\n",
    "        train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "        dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "        print(f\"epoch number: {epoch + 1}, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "        scheduler.step(dev_acc, 'max')\n",
    "\n",
    "        if dev_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_model_gradnorm(para_train, para_dev, sts_train, sts_dev, filepath, alpha, layer):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "    dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "    print(f\"epoch number: 0, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for (para_step, para_batch), (sts_step, sts_batch) in zip(tqdm(get_batches(para_train), desc='train'), tqdm(get_batches(sts_train), desc='train')):\n",
    "            b_para_sentences1, b_para_sentences2, b_para_labels = para_batch['sentence1'], para_batch['sentence2'], para_batch['is_duplicate']\n",
    "            b_sts_sentences1, b_sts_sentences2, b_sts_labels = sts_batch['sentence1'], sts_batch['sentence2'], sts_batch['similarity']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            para_logits = model.forward(b_para_sentences1, b_para_sentences2, 0, device).flatten()\n",
    "            sts_logits = model.forward(b_sts_sentences1, b_sts_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_para_labels = torch.as_tensor(b_para_labels.values, dtype=torch.float32)\n",
    "            b_para_labels = b_para_labels.to(device)\n",
    "            b_sts_labels = torch.as_tensor(b_sts_labels.values, dtype=torch.float32)\n",
    "            b_sts_labels = b_sts_labels.to(device)\n",
    "\n",
    "            loss = (F.binary_cross_entropy(para_logits, b_para_labels, reduction='mean') + F.mse_loss(sts_logits, b_sts_labels, reduction='mean')) / 2\n",
    "            if iters == 0:\n",
    "                weights = torch.ones_like(loss)\n",
    "                weights = torch.nn.Parameter(weights)\n",
    "                T = weights.sum().detach()\n",
    "                optimizer2 = torch.optim.Adam([weights], lr=1e-2)\n",
    "                l0 = loss.detach()\n",
    "            weighted_loss = np.dot(weights, loss)\n",
    "            weighted_loss.backward()\n",
    "            gradient_weights = []\n",
    "            for i in range(len(loss)):\n",
    "                d_l = torch.autograd.grad(weights[i] * loss[i], layer.parameters())[0]\n",
    "                gradient_weights.append(torch.norm(d_l))\n",
    "            gradient_weights = torch.stack(gradient_weights)\n",
    "            lossratio = loss.detach() / l0\n",
    "            r_t = lossratio / lossratio.mean()\n",
    "            avg_gradient_weight = gradient_weights.mean().detach()\n",
    "            const_factor = (avg_gradient_weight * r_t ** alpha).detach()\n",
    "            gradnormloss = torch.abs(gradient_weights - const_factor).sum()\n",
    "            optimizer2.zero_grad()\n",
    "            gradnormloss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer2.step()\n",
    "            weights = (weights / weights.sum() * T).detach()\n",
    "            weights = torch.nn.Parameter(weights)\n",
    "            optimizer2 = torch.optim.Adam([weights], lr=1e-2)\n",
    "            iters += 1\n",
    "\n",
    "        train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "        dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "        print(f\"epoch number: {epoch + 1}, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "        scheduler.step(dev_acc, 'max')\n",
    "\n",
    "        if dev_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_singletask_model(filepath1, filepath2):\n",
    "    train_singletask_para_model(para_train, para_dev, filepath1)\n",
    "    train_singletask_sts_model(sts_train, sts_dev, filepath2)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    para_model = load_model(filepath1, device)\n",
    "    sts_model = load_model(filepath2, device)\n",
    "\n",
    "    para_acc = eval_singletask_model(para_model, device, para_test, 0, 'test')\n",
    "    sts_acc = eval_singletask_model(sts_model, device, sts_test, 1, 'test')\n",
    "\n",
    "    print(f'Final test accuracy. PARA: {para_acc}, STS: {sts_acc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_singletask_model('./models/para_single', './models/sts_single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multitask_model(filepath, pcgrad_flag):\n",
    "    train_multitask_model(para_train, para_dev, sts_train, sts_dev, filepath, pcgrad_flag)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    model = load_model(filepath, device)\n",
    "\n",
    "    para_acc = eval_singletask_model(model, device, para_test, 0, 'test')\n",
    "    sts_acc = eval_singletask_model(model, device, sts_test, 1, 'test')\n",
    "\n",
    "    print(f'Final test accuracy. PARA: {para_acc}, STS: {sts_acc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multitask_model('./models/multitask', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multitask_model('./models/multitask_pcgrad', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
