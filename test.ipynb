{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "for folder in ['multitask', 'para_single', 'sts_single', 'multitask_pcgrad']:\n",
    "    try:\n",
    "        os.mkdir('./models/' + folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "sts_train = pd.read_csv('./data/sts-train.csv', sep=\"\\t\")\n",
    "sts_train = sts_train.dropna()\n",
    "\n",
    "para_train = pd.read_csv('./data/quora-train.csv', sep=\"\\t\")\n",
    "para_train = para_train.dropna()[:len(sts_train)]\n",
    "\n",
    "sts_dev = pd.read_csv('./data/sts-dev.csv', sep=\"\\t\")\n",
    "sts_dev = sts_dev.dropna() \n",
    "\n",
    "para_dev = pd.read_csv('./data/quora-dev.csv', sep=\"\\t\")\n",
    "para_dev = para_dev.dropna()[:len(sts_dev)]\n",
    "\n",
    "sts_combined_set = pd.concat([sts_train, sts_dev], ignore_index=True, axis=0)\n",
    "para_combined_set = pd.concat([para_train, para_dev], ignore_index=True, axis=0)\n",
    "\n",
    "sts_train, sts_dev = train_test_split(sts_combined_set, test_size=0.3, train_size=0.7, shuffle=False)\n",
    "sts_dev, sts_test = train_test_split(sts_dev, test_size=0.5, train_size=0.5, shuffle=False)\n",
    "para_train, para_dev = train_test_split(para_combined_set, test_size=0.3, train_size=0.7, shuffle=False)\n",
    "para_dev, para_test = train_test_split(para_dev, test_size=0.5, train_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cs229/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)\n",
    "# print(para_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "N_PARAPHRASE_CLASSES = 1\n",
    "N_SIMILARITY_CLASSES = 1\n",
    "DROPOUT_PROB = 0.5\n",
    "INPUT_SIZE = 768\n",
    "\n",
    "class NLP_Model(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NLP_Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.paraphrase_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.paraphrase_linear_interact = nn.Linear(INPUT_SIZE, N_PARAPHRASE_CLASSES)\n",
    "        self.similarity_linear = nn.Linear(INPUT_SIZE, INPUT_SIZE // 2)\n",
    "        self.similarity_linear_interact = nn.Linear(INPUT_SIZE, N_SIMILARITY_CLASSES)\n",
    "    \n",
    "    def forward(self, sentences1, sentences2, task, device):\n",
    "        '''\n",
    "        Task 0 is para. Task 1 is similarity.\n",
    "        '''\n",
    "        sentences1 = torch.as_tensor(self.model.encode(sentences1.tolist()))\n",
    "        sentences1 = sentences1.to(device)\n",
    "        sentences2 = torch.as_tensor(self.model.encode(sentences2.tolist()))\n",
    "        sentences2 = sentences2.to(device)\n",
    "        if task == 0:\n",
    "            sentences1 = self.dropout(sentences1)\n",
    "            sentences1 = F.relu(self.paraphrase_linear(sentences1))\n",
    "            sentences2 = self.dropout(sentences2)\n",
    "            sentences2 = F.relu(self.paraphrase_linear(sentences2))\n",
    "            combined = torch.concat((sentences1, sentences2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.sigmoid(self.paraphrase_linear_interact(combined))\n",
    "        if task == 1:\n",
    "            sentences1 = self.dropout(sentences1)\n",
    "            sentences1 = F.relu(self.similarity_linear(sentences1))\n",
    "            sentences2 = self.dropout(sentences2)\n",
    "            sentences2 = F.relu(self.similarity_linear(sentences2))\n",
    "            combined = torch.concat((sentences1, sentences2), dim=-1)\n",
    "            combined = self.dropout(combined)\n",
    "            return F.sigmoid(self.similarity_linear_interact(combined)) * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def save_model(model, optimizer, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, f'{filepath}/model')\n",
    "    model.model.save(f'{filepath}/transformer')\n",
    "    print(f\"saved the model to {filepath}\")\n",
    "\n",
    "def load_model(filepath, device):\n",
    "    with torch.no_grad():\n",
    "        save_info = torch.load(f'{filepath}/model')\n",
    "        transformer_model = SentenceTransformer(f'{filepath}/transformer')\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        model = NLP_Model(transformer_model)\n",
    "        model.load_state_dict(save_info['model'])\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = AdamW(list(model.parameters()) + list(transformer_model.parameters()), lr=1e-4)\n",
    "        optimizer.load_state_dict(save_info['optim'])\n",
    "        \n",
    "        random.setstate(save_info['system_rng'])\n",
    "        np.random.set_state(save_info['numpy_rng'])\n",
    "        torch.random.set_rng_state(save_info['torch_rng'])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def get_batches(dataset, batch_size=512):\n",
    "    \"\"\"\n",
    "    Pass in dataset and batch size.\n",
    "    Get generator which yields batches.\n",
    "    \"\"\"\n",
    "    return enumerate(dataset[i*batch_size:(i+1)*batch_size] for i in range(ceil(dataset.shape[0] / batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "def train_singletask_para_model(para_train, para_dev, filepath):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=1e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_accuracy = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_accuracy = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_accuracy}, para dev accuracy: {dev_para_accuracy}\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for step, batch in tqdm(get_batches(para_train), desc='train'):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['is_duplicate']\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(b_sentences1, b_sentences2, 0, device).flatten()\n",
    "\n",
    "            b_labels = torch.as_tensor(b_labels.values, dtype=torch.float32)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            loss = F.binary_cross_entropy(logits, b_labels, reduction='mean')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_para_accuracy = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_accuracy = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_accuracy}, para dev accuracy: {dev_para_accuracy}\")\n",
    "\n",
    "        scheduler.step(dev_para_accuracy)\n",
    "\n",
    "        if dev_para_accuracy >= best_dev_acc:\n",
    "            best_dev_acc = dev_para_accuracy\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, optimizer, filepath)\n",
    "\n",
    "\n",
    "def train_singletask_sts_model(sts_train, sts_dev, filepath):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    multi-class cross-entropy loss.\n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=5e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for step, batch in tqdm(get_batches(sts_train), desc='train'):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['similarity']\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(b_sentences1, b_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_labels = torch.as_tensor(b_labels.values, dtype=torch.float32)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            loss = F.mse_loss(logits, b_labels, reduction='mean')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        scheduler.step(dev_sts_acc)\n",
    "\n",
    "        if dev_sts_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_sts_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, optimizer, filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training#train-in-native-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_singletask_model(model, device, dataset, task, flag):\n",
    "    '''\n",
    "    given dataloader, 2 task-specific finetuned models, and device\n",
    "    return the accuracy for para and for sts\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        truth = []\n",
    "        predictions = []\n",
    "        for step, batch in tqdm(get_batches(dataset), desc=f\"{flag} eval\"):\n",
    "            b_sentences1, b_sentences2, b_labels = batch['sentence1'], batch['sentence2'], batch['is_duplicate' if task == 0 else 'similarity']\n",
    "            truth.extend(b_labels)\n",
    "            logits = model.forward(b_sentences1, b_sentences2, task, device)\n",
    "            logits = logits.detach().cpu().numpy().flatten()\n",
    "            if task == 0:\n",
    "                new_predictions = np.round(logits).flatten()\n",
    "            else:\n",
    "                new_predictions = logits.flatten()\n",
    "            predictions.extend(new_predictions)\n",
    "        if task == 0:\n",
    "            accuracy = (np.array(truth).flatten() == np.array(predictions).flatten()).mean()\n",
    "        else:\n",
    "            accuracy = (np.round(np.array(truth).flatten()) == np.round(np.array(predictions).flatten())).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgrad import PCGrad\n",
    "\n",
    "def train_multitask_model(para_train, para_dev, sts_train, sts_dev, filepath, pcgrad_flag):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=3e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    if pcgrad_flag: optimizer = PCGrad(optimizer)\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "    dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "    print(f\"epoch number: 0, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for (para_step, para_batch), (sts_step, sts_batch) in zip(tqdm(get_batches(para_train), desc='train'), tqdm(get_batches(sts_train), desc='train')):\n",
    "            b_para_sentences1, b_para_sentences2, b_para_labels = para_batch['sentence1'], para_batch['sentence2'], para_batch['is_duplicate']\n",
    "            b_sts_sentences1, b_sts_sentences2, b_sts_labels = sts_batch['sentence1'], sts_batch['sentence2'], sts_batch['similarity']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            para_logits = model.forward(b_para_sentences1, b_para_sentences2, 0, device).flatten()\n",
    "            sts_logits = model.forward(b_sts_sentences1, b_sts_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_para_labels = torch.as_tensor(b_para_labels.values, dtype=torch.float32)\n",
    "            b_para_labels = b_para_labels.to(device)\n",
    "            b_sts_labels = torch.as_tensor(b_sts_labels.values, dtype=torch.float32)\n",
    "            b_sts_labels = b_sts_labels.to(device)\n",
    "\n",
    "            loss = (F.binary_cross_entropy(para_logits, b_para_labels, reduction='mean') + F.mse_loss(sts_logits, b_sts_labels, reduction='mean')) / 2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "        dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "        print(f\"epoch number: {epoch + 1}, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "        scheduler.step(dev_acc, 'max')\n",
    "\n",
    "        if dev_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, optimizer, filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask_model_gradnorm(para_train, para_dev, sts_train, sts_dev, filepath, alpha, layer):\n",
    "    '''\n",
    "    use AdamW optimizer.\n",
    "    binary cross-entropy loss for para, multi-class cross-entropy loss for sts, sum loss functions. \n",
    "    make sure to save model at end to a specific path.\n",
    "    '''\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    transformer.to(device)\n",
    "\n",
    "    model = NLP_Model(transformer)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(list(model.parameters()) + list(transformer.parameters()), lr=3e-2) #~SGD with weight decay 0.01\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "    dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "    train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "    dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "    print(f\"epoch number: 0, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "    train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "    dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "    print(f\"epoch number: 0, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        transformer.train()\n",
    "\n",
    "        for (para_step, para_batch), (sts_step, sts_batch) in zip(tqdm(get_batches(para_train), desc='train'), tqdm(get_batches(sts_train), desc='train')):\n",
    "            b_para_sentences1, b_para_sentences2, b_para_labels = para_batch['sentence1'], para_batch['sentence2'], para_batch['is_duplicate']\n",
    "            b_sts_sentences1, b_sts_sentences2, b_sts_labels = sts_batch['sentence1'], sts_batch['sentence2'], sts_batch['similarity']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            para_logits = model.forward(b_para_sentences1, b_para_sentences2, 0, device).flatten()\n",
    "            sts_logits = model.forward(b_sts_sentences1, b_sts_sentences2, 1, device).flatten()\n",
    "\n",
    "            b_para_labels = torch.as_tensor(b_para_labels.values, dtype=torch.float32)\n",
    "            b_para_labels = b_para_labels.to(device)\n",
    "            b_sts_labels = torch.as_tensor(b_sts_labels.values, dtype=torch.float32)\n",
    "            b_sts_labels = b_sts_labels.to(device)\n",
    "\n",
    "            loss = (F.binary_cross_entropy(para_logits, b_para_labels, reduction='mean') + F.mse_loss(sts_logits, b_sts_labels, reduction='mean')) / 2\n",
    "            if iters == 0:\n",
    "                weights = torch.ones_like(loss)\n",
    "                weights = torch.nn.Parameter(weights)\n",
    "                T = weights.sum().detach()\n",
    "                optimizer2 = torch.optim.Adam([weights], lr=1e-3)\n",
    "                l0 = loss.detach()\n",
    "            weighted_loss = np.dot(weights, loss)\n",
    "            weighted_loss.backward()\n",
    "            gradient_weights = []\n",
    "            for i in range(len(loss)):\n",
    "                d_l = torch.autograd.grad(weights[i] * loss[i], layer.parameters())[0]\n",
    "                gradient_weights.append(torch.norm(d_l))\n",
    "            gradient_weights = torch.stack(gradient_weights)\n",
    "            lossratio = loss.detach() / l0\n",
    "            r_t = lossratio / lossratio.mean()\n",
    "            avg_gradient_weight = gradient_weights.mean().detach()\n",
    "            const_factor = (avg_gradient_weight * r_t ** alpha).detach()\n",
    "            gradnormloss = torch.abs(gradient_weights - const_factor).sum()\n",
    "            optimizer2.zero_grad()\n",
    "            gradnormloss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer2.step()\n",
    "            weights = (weights / weights.sum() * T).detach()\n",
    "            weights = torch.nn.Parameter(weights)\n",
    "            optimizer2 = torch.optim.Adam([weights], lr=1e-3)\n",
    "            iters += 1\n",
    "\n",
    "        train_para_acc = eval_singletask_model(model, device, para_train, 0, 'train')\n",
    "        dev_para_acc = eval_singletask_model(model, device, para_dev, 0, 'dev')\n",
    "        train_sts_acc = eval_singletask_model(model, device, sts_train, 1, 'train')\n",
    "        dev_sts_acc = eval_singletask_model(model, device, sts_dev, 1, 'dev')\n",
    "        print(f\"epoch number: {epoch + 1}, para train accuracy: {train_para_acc}, para dev accuracy: {dev_para_acc}, sts train accuracy: {train_sts_acc}, sts dev accuracy: {dev_sts_acc}\")\n",
    "\n",
    "        train_acc = (train_para_acc + train_sts_acc) / 2\n",
    "        dev_acc = (dev_para_acc + dev_sts_acc) / 2\n",
    "        print(f\"epoch number: {epoch + 1}, avg train accuracy: {train_acc}, avg dev accuracy: {dev_acc}\")\n",
    "\n",
    "        scheduler.step(dev_acc, 'max')\n",
    "\n",
    "        if dev_acc >= best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            print('New best model. Saving.')\n",
    "            save_model(model, optimizer, filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_singletask_model(filepath1, filepath2):\n",
    "    train_singletask_para_model(para_train, para_dev, filepath1)\n",
    "    train_singletask_sts_model(sts_train, sts_dev, filepath2)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    para_model, _ = load_model(filepath1, device)\n",
    "    sts_model, _ = load_model(filepath2, device)\n",
    "\n",
    "    para_acc = eval_singletask_model(para_model, device, para_test, 0, 'test')\n",
    "    sts_acc = eval_singletask_model(sts_model, device, sts_test, 1, 'test')\n",
    "\n",
    "    print(f'Final test accuracy. PARA: {para_acc}, STS: {sts_acc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train eval: 12it [00:14,  1.20s/it]\n",
      "dev eval: 1it [00:01,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0, sts train accuracy: 0.2228476821192053, sts dev accuracy: 0.2691415313225058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 12it [00:14,  1.21s/it]\n",
      "train eval: 12it [00:14,  1.21s/it]\n",
      "dev eval: 1it [00:01,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1, sts train accuracy: 0.2251655629139073, sts dev accuracy: 0.26218097447795824\n",
      "New best model. Saving.\n",
      "saved the model to ./models/sts_single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 12it [00:14,  1.21s/it]\n",
      "train eval: 12it [00:14,  1.22s/it]\n",
      "dev eval: 1it [00:01,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 2, sts train accuracy: 0.24884105960264902, sts dev accuracy: 0.2691415313225058\n",
      "New best model. Saving.\n",
      "saved the model to ./models/sts_single\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 12it [00:14,  1.22s/it]\n",
      "train eval: 12it [00:14,  1.22s/it]\n",
      "dev eval: 1it [00:01,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 3, sts train accuracy: 0.2716887417218543, sts dev accuracy: 0.2505800464037123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 12it [00:14,  1.23s/it]\n",
      "train eval: 12it [00:14,  1.23s/it]\n",
      "dev eval: 1it [00:01,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 4, sts train accuracy: 0.2824503311258278, sts dev accuracy: 0.2459396751740139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 12it [00:14,  1.24s/it]\n",
      "train eval: 12it [00:14,  1.24s/it]\n",
      "dev eval: 1it [00:01,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 5, sts train accuracy: 0.30149006622516555, sts dev accuracy: 0.25754060324825984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 12it [00:14,  1.24s/it]\n",
      "train eval: 12it [00:14,  1.24s/it]\n",
      "dev eval: 1it [00:01,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 6, sts train accuracy: 0.30960264900662254, sts dev accuracy: 0.2505800464037123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 1it [00:01,  1.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_singletask_model(\u001b[39m'\u001b[39;49m\u001b[39m./models/para_single\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./models/sts_single\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mtest_singletask_model\u001b[0;34m(filepath1, filepath2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_singletask_model\u001b[39m(filepath1, filepath2):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# train_singletask_para_model(para_train, para_dev, filepath1)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     train_singletask_sts_model(sts_train, sts_dev, filepath2)\n\u001b[1;32m      5\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     para_model, _ \u001b[39m=\u001b[39m load_model(filepath1, device)\n",
      "Cell \u001b[0;32mIn[11], line 87\u001b[0m, in \u001b[0;36mtrain_singletask_sts_model\u001b[0;34m(sts_train, sts_dev, filepath)\u001b[0m\n\u001b[1;32m     85\u001b[0m b_sentences1, b_sentences2, b_labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39msentence2\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     86\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 87\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(b_sentences1, b_sentences2, \u001b[39m1\u001b[39;49m, device)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     89\u001b[0m b_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(b_labels\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     90\u001b[0m b_labels \u001b[39m=\u001b[39m b_labels\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mNLP_Model.forward\u001b[0;34m(self, sentences1, sentences2, task, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sentences1, sentences2, task, device):\n\u001b[1;32m     24\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    Task 0 is para. Task 1 is similarity.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     sentences1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode(sentences1\u001b[39m.\u001b[39;49mtolist()))\n\u001b[1;32m     28\u001b[0m     sentences1 \u001b[39m=\u001b[39m sentences1\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     sentences2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mencode(sentences2\u001b[39m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m/opt/conda/envs/cs229/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:188\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[39m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    187\u001b[0m             \u001b[39mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 188\u001b[0m                 embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49mcpu()\n\u001b[1;32m    190\u001b[0m         all_embeddings\u001b[39m.\u001b[39mextend(embeddings)\n\u001b[1;32m    192\u001b[0m all_embeddings \u001b[39m=\u001b[39m [all_embeddings[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_singletask_model('./models/para_single', './models/sts_single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multitask_model(filepath, pcgrad_flag):\n",
    "    train_multitask_model(para_train, para_dev, sts_train, sts_dev, filepath, pcgrad_flag)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    model, _ = load_model(filepath, device)\n",
    "\n",
    "    para_acc = eval_singletask_model(model, device, para_test, 0, 'test')\n",
    "    sts_acc = eval_singletask_model(model, device, sts_test, 1, 'test')\n",
    "\n",
    "    print(f'Final test accuracy. PARA: {para_acc}, STS: {sts_acc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multitask_model('./models/multitask', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multitask_model('./models/multitask_pcgrad', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
